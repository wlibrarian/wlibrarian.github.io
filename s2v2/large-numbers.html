
<!DOCTYPE html>

<html lang="en-GB">
<head>
<title>Librarian Online</title>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i" rel="stylesheet"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
});
</script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<link href="../main.css" rel="stylesheet"/>
</head>
<body>
<header>
<h2 class="top" id="top">The Librarian</h2>
<nav>
<a class="nav" href="../index.html">Home</a> /
	<a class="nav" href="../about.html">About</a> /
	<a class="nav" href="../terms.html">Terms</a> /
	<a class="nav" href="../contact.html">Contact</a>
</nav>
</header>
<main>
<h1>Adventures in Recreational Mathematics VII: numbers not astronomic in size</h1><p><b>Isky Mathews</b></p><p><a href="issue.html">Supplementary №2, Volume II</a></p>
<p>This time we will examine some of mathematics’ largest numbers; I feel this is a conceptual area that many find interesting, and even amusing, to think about—some of the numbers I will mention here will be so large that to refer to them as <em>astronomical</em> would not just be inaccurate, since there is no object that could found in these quantities within the observable universe, but, frankly, <em>insulting</em> to their magnitude.</p>
<p>To demonstrate the previous point, we shall begin by considering the number of baryons in the observable universe. Baryons are particles made up of 3 quarks and interact with the strong nuclear force, e.g. protons or neutrons, and we can calculate how many there are using 4 numbers, 3 of which were obtained using data from the Planck Satellite:</p>
<ul>
<li><p><span class="math inline">\(\rho_{crit}\)</span>, the critical density of the universe (<span class="math inline">\(=8.64\times10^{-33}kgm^{-3}\)</span>)</p></li>
<li><p><span class="math inline">\(\Omega_b\)</span>, the fraction of the universe’s energy in baryons (<span class="math inline">\(=0.0485\)</span>)</p></li>
<li><p><span class="math inline">\(L\)</span>, the radius of the observable universe, which is roughly spherical (<span class="math inline">\(=4.39\times10^{26}cm\)</span>)</p></li>
<li><p><span class="math inline">\(m_p\)</span>, the mass of one proton (<span class="math inline">\(=1.67\times10^{-27}kg\)</span>)</p></li>
</ul>
<p>Now, since <span class="math inline">\(\rho_{crit}\)</span> is essentially the energy density of the universe, <span class="math inline">\(\rho_{crit}\times\Omega_b\)</span> is the mass stored in baryons per <span class="math inline">\(cm^3\)</span> of the observable universe on average, making <span class="math inline">\(\rho_{crit}\times\Omega_b\times\frac{4}{3}\pi L^3\)</span> roughly the combined mass of all baryons in the universe. Finally, since a neutron’s mass is essentially equivalent to that of a proton, we divide the above expression by <span class="math inline">\(m_p\)</span> to get</p>
<p><span class="math display">\[\frac{\rho_{crit}\times\Omega_b\times\frac{4}{3}\pi L^3}{m_p} = 8.89\times10^{79}\]</span></p>
<p>which is really quite a big number, in comparison to the numbers of things you encounter for everyday life! However, it was small enough to be expressed, to a fair level of precision and concisely, using a notation we are so familiar with that I barely need to name it: that of the <em>exponential</em>. For many, if asked to write down quickly the biggest number they could think of at the time, exponentials or stacked exponentials<a class="footnote-ref" href="#fn1" id="fnref1"><sup>1</sup></a> would be their first thought, since it’s so simple—for example, just <span class="math inline">\(10^{10^{2}}\)</span> is bigger than the number of baryons in the universe. In fact, our first famous number can be expressed as <span class="math inline">\(10^{100}\)</span>, a <em>googol</em>, and the next as <span class="math inline">\(10^{10^{100}}\)</span>, a <em>googolplex</em>. We shall return to exponentials and the process of stacking them later, for it has great potential to make large numbers.</p>
<h2 id="primitive-recursive-and-non-primitive-recursive-functions">Primitive Recursive and Non-Primitive Recursive functions</h2>
<p>For now, we take ourselves back to near the beginning of the 20th century, where individuals such as <strong>Gödel, Turing</strong> and <strong>Church</strong> were discussing the nature of functions. They realised that the process of calculating the outputs to most functions could be seen as an iterative process that, most importantly, had a predictable number of steps; for example, to calculate <span class="math inline">\(2+2\)</span>, one could see it as applying <span class="math inline">\(f(n) = n+1\)</span> to the input <span class="math inline">\(2\)</span> twice. Such functions were called <em>primitive recursive</em>, because they <em>could</em> be written down or represented recursively, i.e. where they were seen as a series of repeated applications of some function, but could also be written down in a single closed form—all polynomials, exponentials and many more that we are familiar with are primitive recursive. The computer scientist Robert Ackermann is most famous for describing an eponymous function, denoted <span class="math inline">\(A(m,n)\)</span>, that was still possible to evaluate but was not primitive recursive defined by these conditions:</p>
<p><span class="math display">\[A(m,n) = 
\begin{cases}
\text{\(n+1\)} &amp;\quad\text{if \(m=0\)}\\
\text{\(A(m-1,1\))} &amp;\quad\text{if \(m&gt;0\) and \(n=0\)}\\
\text{\(A(m-1, A(m, n-1))\)} &amp;\quad\text{if \(m&gt;0\) and \(n&gt;0\)}
\end{cases}\]</span><br/>
Let us call a <em>closed-form</em> representation of a function a form which uses a finite number of operations and without self reference. Then, an amazing fact is that the Ackermann function’s above self-referential or <em>recursive</em> definition cannot be written out into a closed form, unlike addition or multiplication—this is what it means for it to not be a primitve-recursive function and it grows extremely quickly—try evaluating it for different inputs! Clearly things like <span class="math inline">\(A(0, 3) = 4\)</span> and <span class="math inline">\(A(1, 2) = 4\)</span> are quite small, but then <span class="math inline">\(A(4,3)\)</span> is an incredible 19729 digit number:</p>
<p><span class="math display">\[A(4,3) = 2^{2^{65536}}-3\]</span><br/>
In fact, it’s often difficult to find examples to demonstrate how large the numbers are that the Ackermann function outputs, because nearly all of them are so big that they either can’t be written down in any concise manner or, worse, they couldn’t be computed within the lifetime of the universe given all the computing available today. Furthermore, Ackermann and his peers were later able to show that functions of this kind<a class="footnote-ref" href="#fn2" id="fnref2"><sup>2</sup></a> <em>dominate</em> all primitive recursive functions, i.e. for any primitive recursive function <span class="math inline">\(f(x)\)</span> and a non-primitive-recursive function <span class="math inline">\(g(x)\)</span>, there is some input <span class="math inline">\(n\)</span> so that for all <span class="math inline">\(m&gt;n\)</span>, <span class="math inline">\(g(m) &gt; f(m)\)</span>.</p>
<p>In order to understand and express just <em>how</em> quickly such functions grow, we have to use a lovely typographical system developed some years ago by the famous <strong>Donald Knuth</strong><a class="footnote-ref" href="#fn3" id="fnref3"><sup>3</sup></a> known as <em>up-arrow notation</em>, which is based on the idea of the <em>hyperoperation hierarchy</em>. The first operator in the hierarchy is that of the <em>successor</em>, an unary operator (meaning that it takes 1 argument) which takes in <span class="math inline">\(n\)</span> and outputs <span class="math inline">\(n+1\)</span>, often written <span class="math inline">\(n++\)</span>.<a class="footnote-ref" href="#fn4" id="fnref4"><sup>4</sup></a> Addition can be seen as repeated successorship in that <span class="math inline">\(a+b\)</span> can be seen as denoting <span class="math inline">\(a++\)</span>, <span class="math inline">\(b\)</span> times. Multiplication can then be seen as repeated addition in that <span class="math inline">\(a\times b\)</span> represents <span class="math inline">\(a+a\)</span>, <span class="math inline">\(b\)</span> times. This continues as we go higher up the hierarchy, with each <span class="math inline">\(n\)</span>th operation <span class="math inline">\(a*_nb\)</span> representing performing the <span class="math inline">\((n-1)\)</span>th operation to <span class="math inline">\(a\)</span> by itself, <span class="math inline">\(b\)</span> times. Knuth created the hyperoperation-notation <span class="math inline">\(a\uparrow b\)</span> which <em>starts</em> at exponentiation (as in <span class="math inline">\(a\uparrow b = a^b\)</span>) and by writing more arrows, one goes up the hierarchy, so <span class="math inline">\(2 \uparrow \uparrow 4 = 2^{2^{2^{2}}}\)</span>—the name we give for this operation above exponentiation is "tetration" and <span class="math inline">\(a \uparrow \uparrow \uparrow b\)</span> is called "<span class="math inline">\(a\)</span> <em>pentated</em> by <span class="math inline">\(b\)</span>" etc. These operations make writing really large numbers simple and if we <em>index</em> the arrows, that is say that <span class="math inline">\(\uparrow^{n}\)</span> denotes <span class="math inline">\(n\)</span> arrows, then we can write down numbers that could never have any practical use—for example, the famous <strong>Graham’s number</strong>.</p>
<h2 id="grahams-number">Graham’s Number</h2>
<p>This number comes out of a question in a somewhat ill-defined area of mathematics known as Ramsey Theory, which purports to comprehend the conditions under which complex structures are forced to appear; <strong>Ronald Graham</strong> and <strong>Bruce Lee Rothschild</strong>, both legends in this field, came up with the question in 1970. The question requires understanding what a <em>graph</em> is in pure mathematics; Benedict Randall Shaw has written a helpful article explaining graph theory in a previous issue of <em>The Librarian</em><a class="footnote-ref" href="#fn5" id="fnref5"><sup>5</sup></a>, but a summary is that any set of points and lines drawn connecting them is a graph. More formally, a graph is a set of points along with a set of pairings defining connections between those points—thus neither the precise coordinate/relative position of points nor the shape of the lines connecting them matters, only the connections<a class="footnote-ref" href="#fn6" id="fnref6"><sup>6</sup></a>.</p>
<figure>
<img alt="These three 2-point graphs are the same." src="graph.png" style="width:20.0%"/><figcaption>These three 2-point graphs are the same.</figcaption>
</figure>
<p>Given <span class="math inline">\(n\)</span> points, the graph obtained by adding all possible connections between them is called the <em>complete graph on <span class="math inline">\(n\)</span> vertices</em>, denoted <span class="math inline">\(K_n\)</span> (e.g. <span class="math inline">\(K_3\)</span> is like a triangle and <span class="math inline">\(K_4\)</span> is like a square with its diagonals drawn in). Now, Rothschild and Graham were considering complete graphs of <span class="math inline">\(n\)</span>-dimensional cubes<a class="footnote-ref" href="#fn7" id="fnref7"><sup>7</sup></a>, which have <span class="math inline">\(2^n\)</span> vertices each, and properties of the <em>colourings</em> of their edges, i.e. the ways in which you can assign different colours to those edges. In particular, they asked what was the smallest value of <span class="math inline">\(n\)</span> such that every 2-colour colouring, using, for example, red and blue, of the edges of the complete graph on an <span class="math inline">\(n\)</span>-dimensional cube is <em>forced</em> to contain a subset <span class="math inline">\(S\)</span> containing exactly 4 of its points such that all the edges between the points in <span class="math inline">\(S\)</span> are the same colour and such that all points in <span class="math inline">\(S\)</span> are <em>coplanar</em><a class="footnote-ref" href="#fn8" id="fnref8"><sup>8</sup></a>. They were able to prove that there is such an <span class="math inline">\(n\)</span> and they knew from checking on paper that <span class="math inline">\(n&gt;5\)</span> and so they sought to also put an upper-bound on it (Graham’s number)<a class="footnote-ref" href="#fn9" id="fnref9"><sup>9</sup></a>. It is constructed as follows:</p>
<ul>
<li><p>Let <span class="math inline">\(G_1 = 3 \uparrow^4 3\)</span> (an amazingly large number, so big that the number of 3s in its power-tower representation couldn’t be written in base 10 even if each digit could be assigned to each planck-volume in the observable universe!)</p></li>
<li><p>For each <span class="math inline">\(n\)</span>, let <span class="math inline">\(G_{n+1} = 3\uparrow^{G_n}3\)</span></p></li>
<li><p>Then Graham’s number is <span class="math inline">\(G_{64}\)</span>.</p></li>
</ul>
<p>It is clear from this that uparrow notation becomes inadequate for integers as large as Graham’s Number, since there is no way of expressing it concisely if we need to write out all the arrows. Thus, when you have gotten over <span class="math inline">\(G_{64}\)</span>, we must move on to a better framework that will allow us to see just how large it is "in the grand scheme of things".</p>
<h2 class="unnumbered unnumbered" id="the-fast-growing-hierarchy-or-the-grandest-of-schemes-of-things">The Fast-Growing Hierarchy <em>or</em> the Grandest of Schemes of Things</h2>
<p>The fast-growing hierarchy is a series of functions, built recursively, that grow faster and faster as we go up. We start with the simple function <span class="math inline">\(f_0(x) := x+1\)</span> and we say<a class="footnote-ref" href="#fn10" id="fnref10"><sup>10</sup></a> that <span class="math inline">\(f_1(x) := f_0^x(x)\)</span>, or in other words <span class="math inline">\(x+x\)</span>. Similarly, <span class="math inline">\(f_2(x) := f_1^x(x) = x\times x\)</span> and in general for any integer <span class="math inline">\(n&gt;0\)</span>, <span class="math inline">\(f_n(x) = f_{n-1}^x(x)\)</span>.</p>
<p>So far, there is no difference between this and hyperoperations but now, we can use <em>ordinals</em> to give us unbounded growth-rates…There was a previous article<a class="footnote-ref" href="#fn11" id="fnref11"><sup>11</sup></a> introducing readers to the wonderful universe of ordinals but, to simplify their technical definition, they are a clever set-theoretic version of numbers, discovered by <strong>Georg Cantor</strong>, which essentially allows us to have a natural extension of the integers to varying sizes of infinity. The number <span class="math inline">\(\omega\)</span> is the ordinal "larger" than all the integers but then we still have a well-defined concept of <span class="math inline">\(\omega+1\)</span> or <span class="math inline">\(+2\)</span> or <span class="math inline">\(+n\)</span> and much, much more. We call <span class="math inline">\(\omega\)</span> the first <em>limit ordinal</em>, meaning that it has no specific predecessor, but rather can be reached as a limit of a strictly increasing sequence, and we call <span class="math inline">\(2, 3, 4,n, \ldots\)</span> and <span class="math inline">\(\omega+1, \omega+2,\omega+n\)</span> etc. <em>successor ordinals</em> because they <em>do</em> have a well-defined predecessor (i.e. they are the successor of some known ordinal). Thus we have the definition that if <span class="math inline">\(\alpha\)</span> is a successor ordinal, then <span class="math inline">\(f_\alpha(x) = f_{\alpha-1}^x(x)\)</span>, and if <span class="math inline">\(\alpha\)</span> is a limit ordinal and <span class="math inline">\(S_\alpha\)</span> is a strictly-increasing sequence of ordinals whose limit is <span class="math inline">\(\alpha\)</span> (as in, <span class="math inline">\(\alpha\)</span> is the smallest upper-bound for all the terms in <span class="math inline">\(S_\alpha\)</span>), with <span class="math inline">\(S_{\alpha} [n]\)</span> denoting the <span class="math inline">\(n\)</span>th term of <span class="math inline">\(S_\alpha\)</span> for some ordinal <span class="math inline">\(n\)</span>, then <span class="math inline">\(f_\alpha(x) = f_{S_{\alpha}}[x](x)\)</span>.</p>
<p>To give an example<a class="footnote-ref" href="#fn12" id="fnref12"><sup>12</sup></a>, <span class="math inline">\(f_\omega(x) = f_x(x)\)</span>, since the sequence of integers 1,2,3,…,<span class="math inline">\(x\)</span>,…has the limit <span class="math inline">\(\omega\)</span> but since <span class="math inline">\(\omega+1\)</span> is a successor ordinal, <span class="math inline">\(f_{\omega+1}=f_\omega^x(x)\)</span>. We can observe from these definitions immediately that <span class="math inline">\(f_\omega(x)\)</span> can’t be primitive-recursive, since it grows faster than any <span class="math inline">\(f_n\)</span> for integer <span class="math inline">\(n\)</span>, and thus that it is, in a sense, <em>beyond uparrows</em>, since it can’t be represented in the form <span class="math inline">\(m \uparrow^k x\)</span>, where <span class="math inline">\(m,k\)</span> are fixed integers. In fact, it is possible to show that <span class="math inline">\(f_\omega(x)\)</span> grows at almost exactly the same rate as the Ackermann function that we’ve seen previously and that <span class="math inline">\(f_{\omega+1}(64) &gt; G_{64}\)</span>.<a class="footnote-ref" href="#fn13" id="fnref13"><sup>13</sup></a> Now, you can choose your favourite transfinite ordinal and create a function that grows faster than you can imagine, for example <span class="math inline">\(f_{\omega\times2}, f_{\omega^2}, f_{\omega^\omega}\)</span> or, if <span class="math inline">\(\epsilon_0 = \omega^{\omega^{\omega^{\omega^{\ldots}}}}\)</span>, then you can even have <span class="math inline">\(f_{\epsilon_0}\)</span> and larger!</p>

<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><p>that is to say, those of the form <span class="math display">\[a^{b^{c^{d^{\ldots}}}}\]</span><a class="footnote-back" href="#fnref1">↩</a></p></li>
<li id="fn2"><p>As in, those that can be evaluated in a finite amount of time but that are not primitive recursive.<a class="footnote-back" href="#fnref2">↩</a></p></li>
<li id="fn3"><p>A computer scientist and mathematician, perhaps most famous for his remarkably complicated series of volumes <em>The Art of Computer Programming</em> (often referred to as the computer scientist’s bible!) but also for the typesetting system TeX, whose offspring, , this very publication uses to format its articles!<a class="footnote-back" href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This is, interestingly, why C++ is called what it is—it was supposed to be the <em>successor to C</em><a class="footnote-back" href="#fnref4">↩</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="RandallShawIntroductionGraphTheory17">Benedict Randall Shaw, “An Introduction to Graph Theory,” <em>The Librarian Supplement</em> 1, no. 2 (November 7, 2017), <a class="uri" href="https://librarian.cf/s2v1/graphtheory.html">https://librarian.cf/s2v1/graphtheory.html</a></span>.<a class="footnote-back" href="#fnref5">↩</a></p></li>
<li id="fn6"><p>To be precise, we consider two graphs that have the same number of vertices and the same connections between those vertices but are drawn differently to be distinct graphs or objects but we say they are <em>isomorphic</em>, i.e. share all the same graph-theoretic properties.<a class="footnote-back" href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Benedict Randall Shaw, the mathematic editor, has produced a diagram of such an hypercube in four dimensions, that has been reproduced on the front cover.<a class="footnote-back" href="#fnref7">↩</a></p></li>
<li id="fn8"><p>i.e. are points on a common plane.<a class="footnote-back" href="#fnref8">↩</a></p></li>
<li id="fn9"><p>It may be of interest that subsequently we have created a better bound, <span class="math inline">\(2\uparrow\uparrow\uparrow6\)</span>.<a class="footnote-back" href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Here, <span class="math inline">\(g^n(x)\)</span>, for some integer <span class="math inline">\(n\)</span> and some function <span class="math inline">\(g(x)\)</span>, denotes performing <span class="math inline">\(g\)</span> to the input <span class="math inline">\(x\)</span>, <span class="math inline">\(n\)</span> times.<a class="footnote-back" href="#fnref10">↩</a></p></li>
<li id="fn11"><p><span class="citation" data-cites="MathewsAdventuresRecreationalMathematics17">Isky Mathews, “Adventures in Recreational Mathematics V: Cantor’s Attic,” <em>The Librarian Supplement</em> 1, no. 1 (October 9, 2017), <a class="uri" href="https://librarian.cf/">https://librarian.cf/</a></span>.<a class="footnote-back" href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Some may notice that this definition only applies for integer <span class="math inline">\(x\)</span> (since there is no <span class="math inline">\(3.2\)</span>th function in our list, for example)—that’s because of the caveat that the fast-growing hiearchy only contains functions defined for ordinal inputs.<a class="footnote-back" href="#fnref12">↩</a></p></li>
<li id="fn13"><p>They aren’t actually comparable in size, since <span class="math inline">\(f_{\omega+1}(64) &gt; f_\omega^{64}(6) &gt; G_{64}\)</span>.<a class="footnote-back" href="#fnref13">↩</a></p></li>
</ol>
</section>
</main>
<footer>
<p>In using this webpage, you agree to our <a href="https://librarian.cf/terms.html">terms</a> of use.</p>
</footer>
</body>
</html>

