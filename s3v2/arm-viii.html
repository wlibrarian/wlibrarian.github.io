
<!DOCTYPE html>

<html lang="en-GB">
<head>
<title>TLO · Isky Mathews · Adventures in Recreational Mathematics VIII—in full: Numbers that are not astronomical</title>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i" rel="stylesheet"/>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
});
</script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<link href="../main.css" rel="stylesheet"/>
</head>
<body>
<header>
<h2 class="top" id="top"><i>The Librarian</i> over TCP/IP</h2>
<nav>
<a class="nav" href="../index.html">Home</a> /
	<a class="nav" href="../about.html">About</a> /
	<a class="nav" href="../terms.html">Terms</a> /
	<a class="nav" href="../contact.html">Contact</a>
</nav>
</header>
<main>
<h1>Adventures in Recreational Mathematics VIII—in full: Numbers that are not astronomical</h1><p><b>Isky Mathews</b></p><p><a href="issue.html">Supplement №3, Volume II</a></p>
<p>This time we will examine some of mathematics’ largest numbers; I feel this is a conceptual area that many find interesting, and even amusing, to think about—some of the numbers I will mention here will be so large that to refer to them as <em>astronomical</em> would not just be inaccurate, since there is no object that could found in these quantities within the observable universe, but, frankly, <em>insulting</em> to their magnitude.</p>
<p>To demonstrate the previous point, we shall begin by considering the number of baryons in the observable universe. Baryons are particles made up of 3 quarks and interact with the strong nuclear force, e.g. protons or neutrons, and we can calculate how many there are using 4 numbers, 3 of which were obtained using data from the Planck Satellite:</p>
<ul>
<li><p><span class="math inline">\(\rho_{crit}\)</span>, the critical density of the universe (<span class="math inline">\(=8.64\times10^{-33}kgm^{-3}\)</span>)</p></li>
<li><p><span class="math inline">\(\Omega_b\)</span>, the fraction of the universe’s energy in baryons (<span class="math inline">\(=0.0485\)</span>)</p></li>
<li><p><span class="math inline">\(L\)</span>, the radius of the observable universe, which is roughly spherical (<span class="math inline">\(=4.39\times10^{26}cm\)</span>)</p></li>
<li><p><span class="math inline">\(m_p\)</span>, the mass of one proton (<span class="math inline">\(=1.67\times10^{-27}kg\)</span>)</p></li>
</ul>
<p>Now, as <span class="math inline">\(\rho_{crit}\)</span> is essentially the energy density of the universe, <span class="math inline">\(\rho_{crit}\times\Omega_b\)</span> is the mass stored in baryons per <span class="math inline">\(cm^3\)</span> of the observable universe on average, making <span class="math inline">\(\rho_{crit}\times\Omega_b\times\frac{4}{3}\pi L^3\)</span> roughly the combined mass of all baryons in the universe. Finally, because a neutron’s mass is essentially equivalent to that of a proton, we divide the above expression by <span class="math inline">\(m_p\)</span> to get</p>
<p><span class="math display">\[\frac{\rho_{crit}\times\Omega_b\times\frac{4}{3}\pi L^3}{m_p} = 8.89\times10^{79}\]</span></p>
<p>which is really quite a big number, in comparison to the numbers of things you encounter in everyday life. However, it was small enough to be expressed, to a fair level of precision and concisely, using a notation with which we are so familiar that I barely need to name it: that of the <em>exponential</em>. For many, if asked to write down quickly the biggest number they could think of at the time, exponentials or stacked exponentials of the form</p>
<p><span class="math display">\[a^{b^{c^{d^{\ldots}}}}\]</span></p>
<p>would be their first thought, due to its simplicity—for example, just <span class="math inline">\(10^{10^{2}}\)</span> is more than the number of baryons in the universe. In fact, our first famous number can be expressed as <span class="math inline">\(10^{100}\)</span>, a <em>googol</em>, and the next as <span class="math inline">\(10^{10^{100}}\)</span>, a <em>googolplex</em>. We shall return to exponentials and the process of stacking them later, for it has great potential to make large numbers.</p>
<h2 id="primitive-recursive-and-non-primitive-recursive-functions">Primitive Recursive and Non-Primitive Recursive functions</h2>
<p>For now, we take ourselves back to near the beginning of the 20th century, when individuals such as <strong>Gödel, Turing</strong> and <strong>Church</strong> were discussing the nature of functions. They realised that the process of calculating the outputs to most functions could be seen as an iterative process that, most importantly, had a predictable number of steps; for example, to calculate <span class="math inline">\(2+2\)</span>, one could see it as applying <span class="math inline">\(f(n) = n+1\)</span> to the input <span class="math inline">\(2\)</span> twice. Such functions were called <em>primitive recursive</em>, because they <em>could</em> be written down or represented recursively, i.e. where they were seen as a series of repeated applications of some function, but could also be written down in a single closed form—all polynomials, exponentials and many more that we are familiar with are primitive recursive. The computer scientist Robert Ackermann is most famous for describing an eponymous function, denoted <span class="math inline">\(A(m,n)\)</span>, that was still possible to evaluate but was not primitive recursive, defined by these conditions:</p>
<p><span class="math display">\[A(m,n) = 
\begin{cases}
\text{\(n+1\)} &amp;\quad\text{if \(m=0\)}\\
\text{\(A(m-1,1\))} &amp;\quad\text{if \(m&gt;0\) and \(n=0\)}\\
\text{\(A(m-1, A(m, n-1))\)} &amp;\quad\text{if \(m&gt;0\) and \(n&gt;0\)}
\end{cases}\]</span><br/>
Let us call a <em>closed-form</em> representation of a function a form which uses a finite number of operations and without self reference. Then, an amazing fact is that the Ackermann function’s above self-referential or <em>recursive</em> definition cannot be written out into a closed form, unlike addition or multiplication—this means it is not a primitive-recursive function and it grows extremely quickly—try evaluating it for different inputs! Clearly things like <span class="math inline">\(A(0, 3) = 4\)</span> and <span class="math inline">\(A(1, 2) = 4\)</span> are quite small, but then <span class="math inline">\(A(4,3)\)</span> is an incredible 19729 digit number:</p>
<p><span class="math display">\[A(4,3) = 2^{2^{65536}}-3\]</span><br/>
In fact, it’s often difficult to find examples to demonstrate how large the numbers that the Ackermann function outputs are, because nearly all of them are so big that they either can’t be written down in any concise manner or, worse, they couldn’t be computed within the lifetime of the universe given all the computing available today. Furthermore, Ackermann and his peers were later able to show that functions of this kind<a class="footnote-ref" href="#fn1" id="fnref1"><sup>1</sup></a> <em>dominate</em> all primitive recursive functions, i.e. for any primitive recursive function <span class="math inline">\(f(x)\)</span> and a non-primitive-recursive function <span class="math inline">\(g(x)\)</span>, there is some input <span class="math inline">\(n\)</span> so that for all <span class="math inline">\(m&gt;n\)</span>, <span class="math inline">\(g(m) &gt; f(m)\)</span>.</p>
<p>In order to understand and express just <em>how</em> quickly such functions grow, we have to use a lovely notation developed some years ago by the famous <strong>Donald Knuth</strong><a class="footnote-ref" href="#fn2" id="fnref2"><sup>2</sup></a> known as <em>up-arrow notation</em>, which is based on the idea of the <em>hyperoperation hierarchy</em>. The first operator in the hierarchy is the <em>successor</em>, an unary operator (meaning that it takes 1 argument) which takes in <span class="math inline">\(n\)</span> and outputs <span class="math inline">\(n+1\)</span>, often written <span class="math inline">\(n++\)</span>.<a class="footnote-ref" href="#fn3" id="fnref3"><sup>3</sup></a> Addition can be seen as repeated successorship in that <span class="math inline">\(a+b\)</span> can be seen as denoting <span class="math inline">\(\ldots{}((a++)++)++\ldots{}\)</span>, where the successor operation is applied <span class="math inline">\(b\)</span> times. Similarly, multiplication is repeated addition, as <span class="math inline">\(a\cdot b\)</span> is equal to <span class="math inline">\(a+a\ldots+a\)</span> where <span class="math inline">\(a\)</span> appears <span class="math inline">\(b\)</span> times. We can make a hierarchy of such functions, with <span class="math inline">\(a*_1 b=a\cdot b\)</span> by definition, and thereafter, <span class="math inline">\(a*_n b\)</span> defined as <span class="math inline">\(a*_{n-1}(a*)\)</span> with <span class="math inline">\(b\)</span> instances of <span class="math inline">\(a\)</span> in that definition. Knuth created the hyperoperation-notation <span class="math inline">\(a\uparrow b\)</span> which <em>starts</em> at exponentiation (as in <span class="math inline">\(a\uparrow b = a^b\)</span>) and by writing more arrows, one goes up the hierarchy, so <span class="math inline">\(2 \uparrow \uparrow 4 = 2^{2^{2^{2}}}\)</span>—the name we give for this operation above exponentiation is "tetration" and <span class="math inline">\(a \uparrow \uparrow \uparrow b\)</span> is called "<span class="math inline">\(a\)</span> <em>pentated</em> by <span class="math inline">\(b\)</span>" etc. These operations make writing really large numbers simple and if we <em>index</em> the arrows, that is say that <span class="math inline">\(\uparrow^{n}\)</span> denotes <span class="math inline">\(n\)</span> arrows, then we can write down numbers that could never have any practical use—for example, the famous <strong>Graham’s number</strong>.</p>
<h2 id="grahams-number">Graham’s Number</h2>
<p>This number comes out of a question in a somewhat ill-defined area of mathematics known as Ramsey Theory, which purports to comprehend the conditions under which complex structures are forced to appear; <strong>Ronald Graham</strong> and <strong>Bruce Lee Rothschild</strong>, both legends in this field, came up with the question in 1970. The question requires understanding what a <em>graph</em> is in pure mathematics; Benedict Randall Shaw has written a helpful article explaining graph theory in a previous issue of <em>The Librarian</em><a class="footnote-ref" href="#fn4" id="fnref4"><sup>4</sup></a>, but a summary is that any set of points and lines drawn connecting them is a graph. More formally, a graph is a set of points along with a set of pairings defining connections between those points—thus neither the precise coordinate/relative position of points nor the shape of the lines connecting them matters, only the connections<a class="footnote-ref" href="#fn5" id="fnref5"><sup>5</sup></a>.</p>
<figure>
<img alt="These three 2-point graphs are the same." src="graph.png" style="width:10.0%"/><figcaption>These three 2-point graphs are the same.</figcaption>
</figure>
<p>Given <span class="math inline">\(n\)</span> points, the graph obtained by adding all possible connections between them is called the <em>complete graph on <span class="math inline">\(n\)</span> vertices</em>, denoted <span class="math inline">\(K_n\)</span> (e.g. <span class="math inline">\(K_3\)</span> is like a triangle and <span class="math inline">\(K_4\)</span> is like a square with its diagonals drawn in). Now, Rothschild and Graham were considering complete graphs on <span class="math inline">\(n\)</span>-dimensional cubes<a class="footnote-ref" href="#fn6" id="fnref6"><sup>6</sup></a>, which have <span class="math inline">\(2^n\)</span> vertices each, and properties of the <em>colourings</em> of their edges, i.e. the ways in which you can assign different colours to those edges. In particular, they asked what was the smallest value of <span class="math inline">\(n\)</span> such that every 2-colour colouring, using, for example, red and blue, of the edges of the complete graph on an <span class="math inline">\(n\)</span>-dimensional cube is <em>forced</em> to contain a subset <span class="math inline">\(S\)</span> containing exactly 4 of its points such that all the edges between the points in <span class="math inline">\(S\)</span> are the same colour and such that all points in <span class="math inline">\(S\)</span> are <em>coplanar</em><a class="footnote-ref" href="#fn7" id="fnref7"><sup>7</sup></a>. They were able to prove that there is such an <span class="math inline">\(n\)</span>, and they knew from checking on paper that <span class="math inline">\(n&gt;5\)</span>, and so they sought to also put an upper-bound on it (Graham’s number)<a class="footnote-ref" href="#fn8" id="fnref8"><sup>8</sup></a>. It is constructed as follows:</p>
<ul>
<li><p>Let <span class="math inline">\(G_1 = 3 \uparrow^4 3\)</span> (an amazingly large number, so big that the number of 3s in its power-tower representation couldn’t be written in base 10 even if each digit could be assigned to each planck-volume in the observable universe!)</p></li>
<li><p>For each <span class="math inline">\(n\)</span>, let <span class="math inline">\(G_{n+1} = 3\uparrow^{G_n}3\)</span></p></li>
<li><p>Then Graham’s number is <span class="math inline">\(G_{64}\)</span>.</p></li>
</ul>
<p>It is clear from this that uparrow notation becomes inadequate for integers as large as Graham’s Number, since there is no way of expressing it concisely if we need to write out all the arrows. Thus, when you have gotten over <span class="math inline">\(G_{64}\)</span>, we must move on to a better framework that will allow us to see just how large it is "in the grand scheme of things".</p>
<h2 id="the-fast-growing-hierarchy-or-the-grandest-of-schemes-of-things">The Fast-Growing Hierarchy <em>or</em> the Grandest of Schemes of Things</h2>
<p>The fast-growing hierarchy is a series of functions, built recursively, that grow faster and faster as we go up. We start with the simple function <span class="math inline">\(f_0(x) := x+1\)</span> and we say<a class="footnote-ref" href="#fn9" id="fnref9"><sup>9</sup></a> that <span class="math inline">\(f_1(x) := f_0^x(x)\)</span>, or in other words <span class="math inline">\(x+x\)</span>. Similarly, <span class="math inline">\(f_2(x) := f_1^x(x) = x\times x\)</span> and in general for any integer <span class="math inline">\(n&gt;0\)</span>, <span class="math inline">\(f_n(x) = f_{n-1}^x(x)\)</span>.</p>
<p>So far, there is no difference between this and hyperoperations but now, we can use <em>ordinals</em> to give us unbounded growth-rates…There was a previous article<a class="footnote-ref" href="#fn10" id="fnref10"><sup>10</sup></a> introducing readers to the wonderful universe of ordinals but, to simplify their technical definition, they are a clever set-theoretic version of numbers, discovered by <strong>Georg Cantor</strong>, which essentially allows us to have a natural extension of the integers to varying sizes of infinity. The number <span class="math inline">\(\omega\)</span> is the ordinal "larger" than all the integers but then we still have a well-defined concept of <span class="math inline">\(\omega+1\)</span> or <span class="math inline">\(+2\)</span> or <span class="math inline">\(+n\)</span> and much, much more. We call <span class="math inline">\(\omega\)</span> the first <em>limit ordinal</em>, meaning that it has no specific predecessor, but rather can be reached as a limit of a strictly increasing sequence, and we call <span class="math inline">\(2, 3, 4,n, \ldots\)</span> and <span class="math inline">\(\omega+1, \omega+2,\omega+n\)</span> etc. <em>successor ordinals</em> because they <em>do</em> have a well-defined predecessor (i.e. they are the successor of some known ordinal). Thus we have the definition that if <span class="math inline">\(\alpha\)</span> is a successor ordinal, then <span class="math inline">\(f_\alpha(x) = f_{\alpha-1}^x(x)\)</span>, and if <span class="math inline">\(\alpha\)</span> is a limit ordinal and <span class="math inline">\(S_\alpha\)</span> is a strictly-increasing sequence of ordinals whose limit is <span class="math inline">\(\alpha\)</span> (as in, <span class="math inline">\(\alpha\)</span> is the smallest upper-bound for all the terms in <span class="math inline">\(S_\alpha\)</span>), with <span class="math inline">\(S_{\alpha} [n]\)</span> denoting the <span class="math inline">\(n\)</span>th term of <span class="math inline">\(S_\alpha\)</span> for some ordinal <span class="math inline">\(n\)</span>, then <span class="math inline">\(f_\alpha(x) = f_{S_{\alpha[x]}}(x)\)</span>.</p>
<p>To give an example<a class="footnote-ref" href="#fn11" id="fnref11"><sup>11</sup></a>, <span class="math inline">\(f_\omega(x) = f_x(x)\)</span>, since the sequence of integers 1,2,3,…,<span class="math inline">\(x\)</span>,…has the limit <span class="math inline">\(\omega\)</span> but since <span class="math inline">\(\omega+1\)</span> is a successor ordinal, <span class="math inline">\(f_{\omega+1}=f_\omega^x(x)\)</span>. We can observe from these definitions immediately that <span class="math inline">\(f_\omega(x)\)</span> can’t be primitive-recursive, since it grows faster than any <span class="math inline">\(f_n\)</span> for integer <span class="math inline">\(n\)</span>, and thus that it is, in a sense, <em>beyond uparrows</em>, since it can’t be represented in the form <span class="math inline">\(m \uparrow^k x\)</span>, where <span class="math inline">\(m,k\)</span> are fixed integers. In fact, it is possible to show that <span class="math inline">\(f_\omega(x)\)</span> grows at almost exactly the same rate as the Ackermann function that we’ve seen previously and that <span class="math inline">\(f_{\omega+1}(64) &gt; G_{64}\)</span>.<a class="footnote-ref" href="#fn12" id="fnref12"><sup>12</sup></a> Now, you can choose your favourite transfinite ordinal and create a function that grows faster than you can imagine, for example <span class="math inline">\(f_{\omega\times2}, f_{\omega^2}, f_{\omega^\omega}\)</span> or, if <span class="math inline">\(\epsilon_0 = \omega^{\omega^{\omega^{\omega^{\ldots}}}}\)</span>, then you can even have <span class="math inline">\(f_{\epsilon_0}\)</span> and larger!</p>
<h2 id="kruskals-tree-theorem-and-tree3">Kruskal’s Tree Theorem and <span class="math inline">\(TREE(3)\)</span></h2>
<p>Kruskal’s Tree Theorem, conjectured by Andrew Vazsonyi and proved in 1960 by <span>Joseph Kruskal</span> (an influential combinatoricist), is a statement, once again, relating to graphs and to explain it, we need some more vocabulary concerning them.</p>
<p>We say that, given a graph <span class="math inline">\(G\)</span> and a point <span class="math inline">\(p\)</span> in <span class="math inline">\(G\)</span>, if there is a way of starting at <span class="math inline">\(p\)</span> and traversing a finite number of edges (that is greater than 2) to move through a sequence of distinct vertices of <span class="math inline">\(G\)</span> which eventually ends up at <span class="math inline">\(p\)</span> again, then we call such a path a <em>cycle</em> and we call a graph <em>acyclic</em> if it doesn’t doesn’t contain any cycles. <em>Connected graphs</em> are what they sound like—they are graphs with the property that for any two of its vertices, there is always a path between them. Further, we say that a connected, acyclic graph <span class="math inline">\(G\)</span> that is also <em>rooted</em>, i.e. there is one vertex we call the <em>root</em> and every other vertex is considered (and often drawn) "below" that root, is called a <em>tree</em>. This makes sense intuitively and Fig. 1 gives an example of a tree to demonstrate. Similarly, peiople call the vertices at the ends of branches of trees <em>leaves</em> and, if one vertex <span class="math inline">\(v_1\)</span> is closer to the root than <span class="math inline">\(v_2\)</span> and their paths to the root overlap, then we call <span class="math inline">\(v_1\)</span> a <em>parent</em> of <span class="math inline">\(v_2\)</span>.</p>
<p>A <span class="math inline">\(k\)</span>-labeled tree is one where each of the tree’s vertices are assigned one of <span class="math inline">\(k\)</span> "labels", which for our purposes we may consider as colours. The most complicated definition that Kruskal’s Theorem requires us to consider is the idea of a <span class="math inline">\(k\)</span>-labeled tree <span class="math inline">\(T_1\)</span> being <em>homeomorphically embeddable</em> (h.e. from now on) into another <span class="math inline">\(k\)</span>-labeled tree <span class="math inline">\(T_2\)</span>. Given <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>, we say that <span class="math inline">\(T_1\)</span> can be h.e. into <span class="math inline">\(T_2\)</span> if there is a function <span class="math inline">\(p(x)\)</span> that takes as inputs vertices of <span class="math inline">\(T_1\)</span> and outputs vertices of <span class="math inline">\(T_2\)</span> with the properties that</p>
<ul>
<li><p>for every vertex <span class="math inline">\(v\)</span> from <span class="math inline">\(T_1\)</span>, <span class="math inline">\(v\)</span> and <span class="math inline">\(p(v)\)</span> have the same colour;</p></li>
<li><p>for every pair of vertices <span class="math inline">\(v_1, v_2\)</span> from <span class="math inline">\(T_1\)</span> and if <span class="math inline">\(x ANC y\)</span> denotes the closest common parent of two vertices <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, then <span class="math inline">\(p(v_1 ANC v_2)\)</span> has the same colour as <span class="math inline">\(p(v_1) ANC p(V_2)\)</span>.</p></li>
</ul>
<figure>
<img alt="A tree." src="Tree1.png"/><figcaption>A tree.</figcaption>
</figure>
<figure>
<img alt="The left 3-labeled tree is h.e. into the right 3-labeled tree." src="Tree2.png"/><figcaption>The left 3-labeled tree is h.e. into the right 3-labeled tree.</figcaption>
</figure>
<figure>
<img alt="The left 3-labeled tree is h.e. into the right 3-labeled tree." src="Tree3.png"/><figcaption>The left 3-labeled tree is h.e. into the right 3-labeled tree.</figcaption>
</figure>
<p>To illustrate this somewhat abstract definition, you can look at Fig. 3 which gives an example of two <span class="math inline">\(k\)</span>-labeled trees with the first being h.e. into the other. Finally, Kruskal’s Tree Theorem states that for any <span class="math inline">\(k\)</span> and for any infinite sequence of <span class="math inline">\(k\)</span>-labelled trees <span class="math inline">\(T_1, T_2, T_3, ...\)</span>, where each <span class="math inline">\(T_n\)</span> can have at most <span class="math inline">\(n\)</span> vertices, it is true that for some <span class="math inline">\(i\)</span> and some <span class="math inline">\(j&gt;i\)</span>, the tree <span class="math inline">\(T_i\)</span> is h.e. into <span class="math inline">\(T_j\)</span>. While the theorem itself doesn’t allow us to describe large numbers, the mathematician Harvey Friedman made the observation that the theorem allows us to define the function <span class="math inline">\(TREE(n)\)</span>, which returns the length of the <em>longest finite sequence</em> of <span class="math inline">\(n\)</span>-labeled trees such that no <span class="math inline">\(T_i\)</span> is h.e. into a <span class="math inline">\(T_j\)</span> for integers <span class="math inline">\(i\)</span> and <span class="math inline">\(j&gt;i\)</span> and it turns out that <span class="math inline">\(k\)</span>-labellings of trees are amazingly combinatorially rich:</p>
<ul>
<li><p><span class="math inline">\(TREE(1) = 1\)</span>, since <span class="math inline">\(T_1\)</span> can have at most 1 vertex and at most use 1 colour, so it is simply a single coloured point and that is clearly h.e. into any successive trees of the same colour.</p></li>
<li><p><span class="math inline">\(TREE(2) = 3\)</span>, where the sequence begins with a single vertex of Colour 1, then we have a two-vertex tree of Colour 2 and then we have a single vertex of Colour 2—can you see why this is the longest such sequence?</p></li>
<li><p><span class="math inline">\(TREE(3)\)</span> then is so vastly, incredibly large that I struggle to find description for it.</p></li>
</ul>
<p>The first thing we can say is that an extremely lower bound that can be proven for it is <span class="math inline">\(f_\omega^{f_\omega(187196)}(2)\)</span>, which is clearly immensely bigger than <span class="math inline">\(f_{\omega+1}(64)&gt;G_64\)</span>! I shall now attempt to explain how far up one needs to go in the fast-growing hierarchy to reach a function that can rival <span class="math inline">\(TREE(n)\)</span>.</p>
<p>In the 20th century, the mathematician <span>Veblen</span>, along with others, was attempting to create a schema for notating and comparing really large infinite ordinals; he first proved that if one has a function <span class="math inline">\(h(x)\)</span> that takes as inputs and outputs ordinals, that is strictly increasing, and whose value for a limit ordinal <span class="math inline">\(\lambda\)</span> is equivalent to the limit of the sequence <span class="math inline">\(h(a_0), h(a_1), h(a_2), ...\)</span> where the limit of <span class="math inline">\(a_0, a_1, a_2, ...\)</span> is <span class="math inline">\(\lambda\)</span>,<a class="footnote-ref" href="#fn13" id="fnref13"><sup>13</sup></a> then <span class="math inline">\(h(x)\)</span> has fixed points for some ordinals. This remarkable property of the ordinals is part of what makes them so interesting to study for set-theorists—they seem to be "averse" to notation systems for them. Regardless, Veblen created his <em>Veblen hierarchy</em>, a series of functions <span class="math inline">\(\phi_0(x), \phi_1(x), \phi_2(x), ...\)</span> where <span class="math inline">\(\phi_0(x) = \omega^x\)</span> and each <span class="math inline">\(\phi_{n+1}(x)\)</span> is equal to the <span class="math inline">\(x\)</span>th fixed point of <span class="math inline">\(\phi_n(x)\)</span>—take <span class="math inline">\(\phi_1(1)\)</span>, for example, which is the first fixed point of <span class="math inline">\(\omega^{x}\)</span>, i.e. <span class="math inline">\(\epsilon_0\)</span> as discussed previously. Now, the <em>Feferman-Schütte ordinal</em> <span class="math inline">\(\Gamma_0\)</span> is the smallest ordinal <span class="math inline">\(\alpha\)</span> that satisfies the impressive <span class="math inline">\(\phi_\alpha(0) = \alpha\)</span> or, to paraphrase <span>Solomon Feferman</span> himself, <span class="math inline">\(\Gamma_0\)</span> is the smallest ordinal that cannot be reached by starting with 0 and repeatedly using addition and the Veblen hierarchy of functions<a class="footnote-ref" href="#fn14" id="fnref14"><sup>14</sup></a> (in fact, it is very difficult to create notation systems that can describe ordinals above <span class="math inline">\(\Gamma_0\)</span> as well as all those below it).</p>
<p>What we can now explain is that despite the magnitude of <span class="math inline">\(\Gamma_0\)</span>, it is possible to show that the growth rate of <span class="math inline">\(TREE(n)\)</span> is <span>much</span> greater than that of <span class="math inline">\(f_{\Gamma_0}(x)\)</span>. But, ultimately, <span class="math inline">\(TREE(n)\)</span> is <span>piffle</span> in comparison to what comes next—after all, it grows slow enough for it to still be <span>computable</span>...</p>
<h2 id="computability-and-busy-beaver-numbers">Computability and busy beaver numbers</h2>
<p>You recall the distinction between recursive and primitive-recursive functions made by those great minds near the beginning of the 20th century? That result is but one in a bigger theory of computation called, aptly, <span>computability theory</span>. Computability theorists study fundamentally <span>what it means to compute something</span> by examining <span>models of computation</span>.</p>
<p>Mathematicians in this era were attempting to go back to the foundations of mathematics and to <em>formalise</em> them, that is to make them completely logically rigorous and unambiguous—this was because it had been discovered that without such careful thought and by relying on <span>implicit</span> or vague definitions, one can run into paradoxes or questions that don’t have an answer, simply because they don’t <span>mean anything</span>. One of the interesting notions that <span>Church</span> and <span>Turing</span> picked up on this journey was that most people appeared to assume that all functions and questions in mathematics <span>were answerable</span> and simply required "proper definitions" and a great amount of thought—their great insight was that the <span>process</span> of logically working things out or deducing true statements had mathematical properties in and of itself and thus realised that it was possible to create problems that were well defined (as in, they had a unique answer) but were beyond the capability of any proof-system or computational device to solve; such problems are known as <em>uncomputable problems</em>.</p>
<p>Computability theory is such an amazing area of mathematics that it deserves its own article<a class="footnote-ref" href="#fn15" id="fnref15"><sup>15</sup></a> but we shall simplify here to explain just the concepts required for describing large numbers. Alan Turing came up with the concept of <em>Turing machine</em> (<span class="smallcaps">tm</span>s from now on), which are theoretical automata that have a <em>tape</em> that stretches out infinitely in one direction and is divided up into discrete tape-segments, along with a <em>tape-head</em> that is capable of reading and writing symbols on the tape.</p>
<figure>
<img alt="An illustration of a Turing machine." src="TM.png"/><figcaption>An illustration of a Turing machine.</figcaption>
</figure>
<p>It also has a number of <em>states</em> that it can be in and a rulebook that determines, given that the <span class="smallcaps">tm</span> is in the <span class="math inline">\(n\)</span>th state and that the symbol being read by the tape-head is <span class="math inline">\(X\)</span>, what symbol <span class="math inline">\(Y\)</span> the <span class="smallcaps">tm</span> should overwrite <span class="math inline">\(X\)</span> with and whether to then move one tape-segment right or left. It should finally be noted that a <span class="smallcaps">tm</span> has two special states denoted <span class="math inline">\(YES\)</span> or <span class="math inline">\(NO\)</span> which, when reached during some stage of a <span class="smallcaps">tm</span>-computation, cause the <span class="smallcaps">tm</span> to <em>halt</em>, i.e. cease all movement. Turing and Church believed that a Turing machine should be able to perform any algorithm that is well-defined because:</p>
<ul>
<li><p>They (and other mathematicians such as <span>Gödel</span>) had previously created other systems which were supposed to represent computation, such as the <em><span class="math inline">\(\lambda\)</span>-calculus</em> and the set of general recursive functions, and it turned out that the set of problems they could solve were all equivalent.</p></li>
<li><p>They were able to show ways of creating Turing machines that could evaluate many well known algorithms, such as one that could perform primality-checks or could multiply two numbers together.</p></li>
<li><p>They lived to see <span>John von Neumann</span>, a startingly brilliant mind, design modern-day computer architectures using their ideas and create the first computers.</p></li>
</ul>
<p>In fact, in the 1960s and 70s, many individuals tried to create other deterministic systems that they believed might be able to compute more than Turing machines but all were eventually proved to be able to solve the same set of problems—thus, they proposed the <em>Church-Turing thesis</em>: that every sequence of logical steps that were performable by a human would also be performable, theoretically, by a Turing machine, and vice versa; subsequently, any system that was capable of solving the same set of problems as <span class="smallcaps">tm</span>s came to be known as <em>Turing-complete</em>.</p>
<p>However, as I mentioned previously, those problems Turing-complete systems can solve do not encompass <em>all</em> problems, they only contain <em>computable</em> problems. But what, then, would an uncomputable problem look like? Well, Turing made the observation that certain Turing machines are setup such that, after some finite number of steps, they halt in the state <span class="math inline">\(YES\)</span> or <span class="math inline">\(NO\)</span> and others never halt and simply continue moving along their tape indefinitely. In 1936, he published a paper describing the <em>Halting problem</em>’s uncomputability: it is impossible to create a <em>general, finite-time algorithm</em> to decide accurately whether a given <span class="smallcaps">tm</span> halts—he showed this through a clever proof-by-contradiction. To illustrate it, we first remember that, by Turing-completeness, any computability theorem that applies to Turing machines applies to many of the modern-day computer programming languages<a class="footnote-ref" href="#fn16" id="fnref16"><sup>16</sup></a>, so we shall precede by writing our proof in the language of <em>Python</em>. Let us assume, hoping to reach a contradiction, that there was a program defining a function called <em>Halts(<span class="math inline">\(x\)</span>)</em>—this takes in the name of a programmed-function for which you wish to check whether it halts or not and outputs after a finite amount of time either <em>True</em> or <em>False</em> as per the answer. Then consider the function:</p>
<p>For those less familiar with Python, <em>contradiction(x)</em> is designed to halt after a finite amount of time if and only if <em>Halts(<span class="math inline">\(x\)</span>)</em> says that it will run forever and <em>contradiction(x)</em> will loop forever if and only if <em>Halts(<span class="math inline">\(x\)</span>)</em> says that it will halt after a finite amount of time. This function is clearly contradictory and so our initial assumption, that there could exist such a function <em>Halts(<span class="math inline">\(x\)</span>)</em>, is false. Thus, the Halting problem is uncomputable!</p>
<p>An interesting observation, then, made by a researcher at <span>Bell Labs</span> was that, for a given <span class="math inline">\(n\)</span>, it <em>must be true</em> that there is some number <span class="math inline">\(k\)</span> representing the largest number of steps that a <em>halting</em> <span class="math inline">\(n\)</span>-state Turing machine (i.e. a <span class="smallcaps">tm</span> which does eventually terminate its computation) will take before stopping—the researcher called these <span class="smallcaps">tm</span>s <em>Busy Beavers</em> and the corresponding <span class="math inline">\(k\)</span> for each <span class="math inline">\(n\)</span> the <span class="math inline">\(n\)</span>th <em>Busy Beaver number</em> (<span class="math inline">\(BB(n)\)</span>), because they take a long time to stop and the way that they move up and down their tape during their computation reminded him of a beaver making a dam. So, by definition, there is no <span class="math inline">\(n\)</span>-state Turing machine that takes longer than <span class="math inline">\(BB(n)\)</span> steps to halt.</p>
<p>However, he pointed out that there cannot exist a general algorithm to compute <span class="math inline">\(BB(n)\)</span> for each input <span class="math inline">\(n\)</span> because if there was one, then we would have the following finite-time algorithm to solve the Halting problem:</p>
<ul>
<li><p>If the <span class="smallcaps">tm</span> for which we are trying to determine whether it halts or not has <span class="math inline">\(n\)</span> states, compute <span class="math inline">\(BB(n)\)</span>.</p></li>
<li><p>Then, start running the <span class="smallcaps">tm</span> and wait <span class="math inline">\(BB(n)\)</span> steps. If the <span class="smallcaps">tm</span> halts by this time, then we know that it halts. If the <span class="smallcaps">tm</span> does not halt by this time, then we know, by the definition of <span class="math inline">\(BB(n)\)</span>, that it does not halt.</p></li>
</ul>
<p>but such an algorithm cannot exist by the uncomputability of the Halting problem and so <span class="math inline">\(BB(n)\)</span> is an <span>uncomputable function</span>. Further, there cannot be any computable function <span class="math inline">\(g(n)\)</span> which acts as an upper-bound to <span class="math inline">\(BB(n)\)</span> for each <span class="math inline">\(n\)</span>, because if that was true then similarly we could wait <span class="math inline">\(g(n)\)</span> steps to determine whether an <span class="math inline">\(n\)</span>-state <span class="smallcaps">tm</span> halts (any <span class="smallcaps">tm</span> which takes longer than <span class="math inline">\(g(n)\)</span> steps to halt must have already taken more than <span class="math inline">\(BB(n)\)</span> steps, and so cannot halt), giving us again an impossible finite-time algorithm to solve the Halting problem. So, <span class="math inline">\(BB(n)\)</span> isn’t just uncomputable—no computable functions can act as an upper-bound to it. So all the functions we have seen previously—<span class="math inline">\(A(m,n), f_\omega(n), f_{\epsilon_0}(n), f_{\Gamma_0}(n), TREE(n)\)</span>—they all must inherently grow slower than <span class="math inline">\(BB(n)\)</span> simply by the fact that they are <span>computable</span>.</p>
<p>Mathematicians have worked out that <span class="math inline">\(BB(0) = 0, BB(1) = 1, BB(2) = 4, BB(3) = 6\)</span> and that <span class="math inline">\(BB(13)\)</span> simply by trying and examining all <span class="math inline">\(n\)</span>-state Turing machines for <span class="math inline">\(n&lt;5\)</span> but only lower bounds are known beyond this—for example, we know that</p>
<p><span class="math display">\[BB(7) &gt; 10^{10^{10^{10^{18705353}}}}\]</span><br/>
and that</p>
<p><span class="math display">\[(BB(5) &gt; 3^{3^{3^{3^{3^{...}}}}}\]</span><br/>
where the number of threes is 7625597484987.</p>
<p>But now we can go further <span>even than this</span> to functions and numbers so fantastic that I will at the end write down a number that, to my current knowledge, is greater than anything <span>ever written in any mathematics publication, formal or informal</span> as it is something of my own design. Because computability theory is all about imagining theoretical scenarios related to computation that could never happen—for example, we could never build an <span>actual</span> <span class="smallcaps">tm</span>, since it requires an infinitely long tape—one thing we can consider is an <em>oracle</em>, a purely theoretical construct that would allow us to compute certain uncomputable functions. For example, we could have the oracle <span class="math inline">\(BB_0(n)\)</span>, which outputs the <span class="math inline">\(n\)</span>th Busy Beaver number, although such a thing could never normally exist in our universe even if it was infinite but remember, this is <em>pure mathematics</em>—oracles can exist because we <span>say so</span> and they have interesting theoretical properties. For example, if you take the set <span class="math inline">\(S_0\)</span> of <span class="smallcaps">tm</span>s equipped each with <span class="math inline">\(BB_0(n)\)</span>, the analogous version of the halting problem for <span class="math inline">\(S_0\)</span> is still uncomputable by those <span class="smallcaps">tm</span>s in <span class="math inline">\(S_0\)</span> and so <span class="math inline">\(S_0\)</span> has its own associated Busy Beaver function, <span class="math inline">\(BB_1(n)\)</span>, that grows faster than any function computable by <span class="math inline">\(S_0\)</span> <span class="smallcaps">tm</span>s (and thus grows <span>far</span> faster than <span class="math inline">\(BB_0(n)\)</span>). Similarly, we could then introduce an oracle for <span class="math inline">\(BB_1(n)\)</span> and create a new set of Turing machines <span class="math inline">\(S_1\)</span> equipped with both the <span class="math inline">\(BB_1(n), BB_0(n)\)</span> oracles and, once again, <span class="math inline">\(S_1\)</span> would have its own version of the Halting Problem and thus a well-defined, but uncomputable function <span class="math inline">\(BB_2(n)\)</span> that grows even faster than <span class="math inline">\(BB_1(n)\)</span>.</p>
<p>By iteratively creating more and more sets <span class="math inline">\(S_0, S_1, S_2, ...\)</span>, we get faster and faster growing functions assigned to them <span class="math inline">\(B_0(n),B_1(n),B_2(n),...\)</span> and so, as with the fast-growing hierarchy, we can allow for <em>ordinal</em> subscripts where if <span class="math inline">\(\lambda\)</span> is a limit ordinal, then <span class="math inline">\(B_\lambda(n)\)</span> is the Busy Beaver function for Turing machines with oracles for all ordinals below it. Thus, we can have <span class="math inline">\(B_\omega(n)\)</span> and <span class="math inline">\(B_{\epsilon_0}(n)\)</span> etc. Now, we call an ordinal <span class="math inline">\(\alpha\)</span> <em>computable</em> if there is a Turing machine which can tell for any two distinct ordinals <span class="math inline">\(\beta, \gamma\)</span> smaller than <span class="math inline">\(\alpha\)</span> whether <span class="math inline">\(\beta &gt; \gamma\)</span> or <span class="math inline">\(\gamma &gt; \beta\)</span> (for example, <span class="math inline">\(\omega\)</span> is a computable ordinal, since any good computer can tell, given two nonequal integers, which one is bigger than the other). So, I define the <em>Ultra-function</em> <span class="math inline">\(U(n)\)</span> to be <span class="math inline">\(B_{\delta(n)}(n)\)</span>, where <span class="math inline">\(\delta(n)\)</span> is the largest ordinal computable for <span class="math inline">\(n\)</span>-state Turing machines - so <span class="math inline">\(U(n)\)</span> grows faster than any individual <span class="math inline">\(B_\alpha(n)\)</span>!</p>
<p>Thus, as a final answer to <em>Who can name the bigger number?</em>, I advise you to simply write down <span class="math inline">\(K_{\Omega} = U(TREE(TREE(TREE(3))))\)</span>, which is quite concise but which I can verify, thanks to some results on much smaller numbers, is so big that if your opponent attempts to describe another number <span class="math inline">\(N\)</span>, whether the statement <span class="math inline">\(N&gt;K\)</span> is true is in fact <em>independent</em> of <span class="math inline">\(ZFC\)</span>-set-theory (i.e. cannot be <em>proved or disproved</em> using most modern-day mathematical techniques, regardless of whether it seems obvious).</p>
<p>If you enjoyed this article and would interested in reading more, here are some titles of subjects and numbers that you can look up for further reading:</p>
<ul>
<li><p>Rayo’s Number (before I wrote <span class="math inline">\(K_{\Omega}\)</span>’s definition above, it <em>was</em> the biggest number ever written down in human history)</p></li>
<li><p>Chaitin’s constant, the Halting Problem and other uncomputable problems.</p></li>
<li><p>Ordinals and Cardinals in Set Theory.</p></li>
<li><p>Ramsey Theory and Ramsey Numbers.</p></li>
<li><p>Go to the Googology Wiki, a fun website dedicated to listing and describing some of the largest numbers mathematicians know of!</p></li>
</ul>
<p>And now for the challenges. For those who haven’t read ARM before, each article ends with 2 challenges and if you get a solution to either one, please email either <strong>Isky Mathews</strong> (isky.mathews@westminster.org.uk) or <strong>Benedict Randall Shaw</strong> (benedict.randallshaw@westminster.org.uk):</p>
<p><strong>Challenge 1:</strong> Determine whether <span class="math inline">\(10^6\uparrow\uparrow10^6 &gt; 3\uparrow\uparrow\uparrow 3\)</span> or <span class="math inline">\(3\uparrow\uparrow\uparrow 3 &gt;10^6\uparrow\uparrow10^6\)</span>.</p>
<p><strong>Challenge 2:</strong> Can you think of a way of describing bigger numbers than using the "uncomputability hierarchy" above along with some form of (minimally-heuristic) argument as to why you believe they are bigger? (This would be exceedingly impressive, not just to us but to the entire mathematics community.)</p>

<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><p>As in, those that can be evaluated in a finite amount of time but that are not primitive recursive.<a class="footnote-back" href="#fnref1">↩</a></p></li>
<li id="fn2"><p>A computer scientist and mathematician, perhaps most famous for his remarkably complicated series of volumes <em>The Art of Computer Programming</em> (often referred to as the computer scientist’s bible!) but also for the typesetting system TeX, whose offspring, , this very publication uses to format its articles!<a class="footnote-back" href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This is, interestingly, why C++ is called what it is—it was supposed to be the <em>successor to C</em><a class="footnote-back" href="#fnref3">↩</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="RandallShawIntroductionGraphTheory17">Benedict Randall Shaw, “An Introduction to Graph Theory,” <em>The Librarian Supplement</em> 1, no. 2 (November 7, 2017), <a class="uri" href="https://librarian.cf/s2v1/graphtheory.html">https://librarian.cf/s2v1/graphtheory.html</a></span>.<a class="footnote-back" href="#fnref4">↩</a></p></li>
<li id="fn5"><p>To be precise, we consider two graphs that have the same number of vertices and the same connections between those vertices but are drawn differently to be distinct graphs or objects but we say they are <em>isomorphic</em>, i.e. share all the same graph-theoretic properties.<a class="footnote-back" href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Benedict Randall Shaw, the mathematic editor, produced a diagram of such an hypercube in four dimensions that was reproduced on the front cover of a previous issue.<a class="footnote-back" href="#fnref6">↩</a></p></li>
<li id="fn7"><p>i.e. are points on a common plane.<a class="footnote-back" href="#fnref7">↩</a></p></li>
<li id="fn8"><p>It may be of interest that subsequently we have created a better bound, <span class="math inline">\(2\uparrow\uparrow\uparrow6\)</span>.<a class="footnote-back" href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Here, <span class="math inline">\(g^n(x)\)</span>, for some integer <span class="math inline">\(n\)</span> and some function <span class="math inline">\(g(x)\)</span>, denotes performing <span class="math inline">\(g\)</span> to the input <span class="math inline">\(x\)</span>, <span class="math inline">\(n\)</span> times.<a class="footnote-back" href="#fnref9">↩</a></p></li>
<li id="fn10"><p><span class="citation" data-cites="MathewsAdventuresRecreationalMathematics17">Isky Mathews, “Adventures in Recreational Mathematics V: Cantor’s Attic,” <em>The Librarian Supplement</em> 1, no. 1 (October 9, 2017), <a class="uri" href="https://librarian.cf/">https://librarian.cf/</a></span>.<a class="footnote-back" href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Some may notice that this definition only applies for integer <span class="math inline">\(x\)</span> (since there is no <span class="math inline">\(3.2\)</span>th function in our list, for example)—that’s because of the caveat that the fast-growing hiearchy only contains functions defined for ordinal inputs.<a class="footnote-back" href="#fnref11">↩</a></p></li>
<li id="fn12"><p>They aren’t actually comparable in size, since <span class="math inline">\(f_{\omega+1}(64) &gt; f_\omega^{64}(6) &gt; G_{64}\)</span>.<a class="footnote-back" href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Such a function is commonly called a <em>normal function</em> in set theory.<a class="footnote-back" href="#fnref13">↩</a></p></li>
<li id="fn14"><p><span class="citation" data-cites="schutte1977">Kurt Schütte, <em>Proof Theory</em>, vol. 225, Grundlehren Der Mathematischen Wissenschaften (Berlin, Heidelberg: Springer Berlin Heidelberg, 1977), doi:<a href="https://doi.org/10.1007/978-3-642-66473-1">10.1007/978-3-642-66473-1</a></span>.<a class="footnote-back" href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Perhaps a series of articles!<a class="footnote-back" href="#fnref15">↩</a></p></li>
<li id="fn16"><p>If we assume they are being run on a computer of unlimited memory and that they are capable of performing genuine arbitrary-precision arithmetic.<a class="footnote-back" href="#fnref16">↩</a></p></li>
</ol>
</section>
</main>
<footer>
<p>In using this webpage, you agree to our <a href="https://librarian.cf/terms.html">terms</a> of use.</p>
</footer>
</body>
</html>

